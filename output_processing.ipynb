{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone = 0\n",
    "zone_data = pd.read_csv(\"smaller_zone/zone_0/c3_ABLE_1950_08.csv\", usecols=[\"coord_index\", \"time_step\", \"latitude_rank\", \"longitude_rank\"])\n",
    "zone_data = zone_data[zone_data[\"time_step\"]==min(zone_data[\"time_step\"])][[\"latitude_rank\", \"longitude_rank\"]]\n",
    "\n",
    "zone_min_lat = min(zone_data[\"latitude_rank\"])\n",
    "zone_min_lon = min(zone_data[\"longitude_rank\"])\n",
    "zone_data[\"latitude_rank\"] = zone_data[\"latitude_rank\"] - zone_min_lat + 1\n",
    "zone_data[\"longitude_rank\"] = zone_data[\"longitude_rank\"] - zone_min_lon + 1\n",
    "num_lat = max(zone_data[\"latitude_rank\"])\n",
    "zone_data = zone_data.sort_values(by=[\"longitude_rank\", \"latitude_rank\"])\n",
    "zone_data[\"coord_index\"] = (zone_data[\"longitude_rank\"]-1)*num_lat + zone_data[\"latitude_rank\"]\n",
    "zone_data = zone_data.sort_values(by=\"coord_index\")\n",
    "zone_data = zone_data.set_index(\"coord_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_df = pd.read_csv(f\"smaller_nodes/nodes_zone_{zone}.csv\", usecols=[\"node1\", \"node2\"])\n",
    "node1 = node_df[\"node1\"].to_numpy()\n",
    "node2 = node_df[\"node2\"].to_numpy()\n",
    "N_nodes = node2.max()\n",
    "node1_diag = np.concatenate([node1, np.arange(1, N_nodes + 1)])\n",
    "node2_diag = np.concatenate([node2, np.arange(1, N_nodes + 1)])\n",
    "N_edges = len(node1)\n",
    "N_edges_diag = len(node1_diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edges for each node\n",
    "node_edges = []\n",
    "for n in range(1, N_nodes+1):\n",
    "    n_edges = []\n",
    "    for e in range(len(node1)):\n",
    "        if ((n==node1[e]) | (n==node2[e])):\n",
    "            n_edges.append(e+1)\n",
    "    node_edges.append(n_edges)\n",
    "\n",
    "node_edges_diag = []\n",
    "for n in range(1, N_nodes+1):\n",
    "    n_edges_diag = []\n",
    "    for e in range(len(node1_diag)):\n",
    "        if ((n==node1_diag[e]) | (n==node2_diag[e])):\n",
    "            n_edges_diag.append(e+1)\n",
    "    node_edges_diag.append(n_edges_diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_north_edges = []\n",
    "node_east_edges = []\n",
    "node_south_edges = []\n",
    "node_west_edges = []\n",
    "node_vert_edges = []\n",
    "\n",
    "for n in range(1, N_nodes+1):\n",
    "    node_latitude = zone_data.loc[n, \"latitude_rank\"]\n",
    "    node_longitude = zone_data.loc[n, \"longitude_rank\"]\n",
    "    n_north_edges = []\n",
    "    n_east_edges = []\n",
    "    n_south_edges = []\n",
    "    n_west_edges = []\n",
    "    n_vert_edges = []\n",
    "\n",
    "    n_edges = node_edges_diag[n-1]\n",
    "    for e in n_edges:\n",
    "        # Selecting other node in edge\n",
    "        other_node = node1_diag[e-1]\n",
    "        if other_node==n:\n",
    "            other_node = node2_diag[e-1]\n",
    "        other_node_latitude = zone_data.loc[other_node, \"latitude_rank\"]\n",
    "        other_node_longitude = zone_data.loc[other_node, \"longitude_rank\"]\n",
    "        coord_difference = (node_latitude - other_node_latitude, node_longitude - other_node_longitude)\n",
    "        if coord_difference == (1, 0):\n",
    "            n_south_edges.append(e)\n",
    "        elif coord_difference == (-1, 0):\n",
    "            n_north_edges.append(e)\n",
    "        elif coord_difference == (0, 1):\n",
    "            n_west_edges.append(e)\n",
    "        elif coord_difference == (0, -1):\n",
    "            n_east_edges.append(e)\n",
    "        elif coord_difference == (0, 0):\n",
    "            n_vert_edges.append(e)\n",
    "        else:\n",
    "            print(\"something wrong\")\n",
    "    node_north_edges.append(n_north_edges)\n",
    "    node_east_edges.append(n_east_edges)\n",
    "    node_south_edges.append(n_south_edges)\n",
    "    node_west_edges.append(n_west_edges)\n",
    "    node_vert_edges.append(n_vert_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_param(col, edge_map, edge_diag_map):\n",
    "    match = re.match(r\"(w1|w2|p)\\[(\\d+)\\]\", col)\n",
    "    if match:\n",
    "        base, idx = match.groups()\n",
    "        idx = int(idx)\n",
    "        if base in (\"w1\", \"w2\"):\n",
    "            edge_key = edge_map.get(idx)\n",
    "        elif base == \"p\":\n",
    "            edge_key = edge_diag_map.get(idx)\n",
    "        else:\n",
    "            edge_key = None\n",
    "        return f\"{base}[{edge_key}]\" if edge_key else col\n",
    "    return col\n",
    "\n",
    "def safe_mean(draws, columns):\n",
    "    return np.mean(draws[columns].values, axis=1) if columns else np.full(draws.shape[0], np.nan)\n",
    "\n",
    "def safe_abs_mean(draws, columns):\n",
    "    return np.mean(np.abs(draws[columns].values), axis=1) if columns else np.full(draws.shape[0], np.nan)\n",
    "\n",
    "def safe_sum(draws, columns):\n",
    "    return np.sum(draws[columns].values, axis=1) if columns else np.full(draws.shape[0], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_draws(draws):\n",
    "    processed_draws = draws[[\"lp__\", \".chain\", \".iteration\"]].copy()\n",
    "    new_columns = {}\n",
    "    for n in range(1, N_nodes + 1):\n",
    "        n_edges = node_edges[n-1]\n",
    "        n_edges_diag = node_edges_diag[n-1]\n",
    "        n_north_edges = node_north_edges[n-1]\n",
    "        n_east_edges = node_east_edges[n-1]\n",
    "        n_south_edges = node_south_edges[n-1]\n",
    "        n_west_edges = node_west_edges[n-1]\n",
    "        n_vert_edges = node_vert_edges[n-1]\n",
    "\n",
    "        total_w1 = [f\"w1[{e}]\" for e in n_edges if f\"w1[{e}]\" in draws.columns]\n",
    "        total_w2 = [f\"w2[{e}]\" for e in n_edges if f\"w2[{e}]\" in draws.columns]\n",
    "        total_p = [f\"p[{e}]\" for e in n_edges_diag if f\"p[{e}]\" in draws.columns]\n",
    "\n",
    "        north_w1 = [f\"w1[{e}]\" for e in n_north_edges if f\"w1[{e}]\" in draws.columns]\n",
    "        east_w1 = [f\"w1[{e}]\" for e in n_east_edges if f\"w1[{e}]\" in draws.columns]\n",
    "        south_w1 = [f\"w1[{e}]\" for e in n_south_edges if f\"w1[{e}]\" in draws.columns]\n",
    "        west_w1 = [f\"w1[{e}]\" for e in n_west_edges if f\"w1[{e}]\" in draws.columns]\n",
    "\n",
    "        north_w2 = [f\"w2[{e}]\" for e in n_north_edges if f\"w2[{e}]\" in draws.columns]\n",
    "        east_w2 = [f\"w2[{e}]\" for e in n_east_edges if f\"w2[{e}]\" in draws.columns]\n",
    "        south_w2 = [f\"w2[{e}]\" for e in n_south_edges if f\"w2[{e}]\" in draws.columns]\n",
    "        west_w2 = [f\"w2[{e}]\" for e in n_west_edges if f\"w2[{e}]\" in draws.columns]\n",
    "\n",
    "        north_p = [f\"p[{e}]\" for e in n_north_edges if f\"p[{e}]\" in draws.columns]\n",
    "        east_p = [f\"p[{e}]\" for e in n_east_edges if f\"p[{e}]\" in draws.columns]\n",
    "        south_p = [f\"p[{e}]\" for e in n_south_edges if f\"p[{e}]\" in draws.columns]\n",
    "        west_p = [f\"p[{e}]\" for e in n_west_edges if f\"p[{e}]\" in draws.columns]\n",
    "        vert_p = [f\"p[{e}]\" for e in n_vert_edges if f\"p[{e}]\" in draws.columns]\n",
    "\n",
    "        new_columns[f\"tot_w1[{n}]\"] = safe_mean(draws, total_w1)\n",
    "        new_columns[f\"tot_w2[{n}]\"] = safe_mean(draws, total_w2)\n",
    "        new_columns[f\"tot_p[{n}]\"] = safe_mean(draws, total_p)\n",
    "\n",
    "        new_columns[f\"tot_abs_w1[{n}]\"] = safe_abs_mean(draws, total_w1)\n",
    "        new_columns[f\"tot_abs_w2[{n}]\"] = safe_abs_mean(draws, total_w2)\n",
    "        new_columns[f\"tot_abs_p[{n}]\"] = safe_abs_mean(draws, total_p)\n",
    "\n",
    "        new_columns[f\"north_w1[{n}]\"] = safe_sum(draws, north_w1)\n",
    "        new_columns[f\"east_w1[{n}]\"] = safe_sum(draws, east_w1)\n",
    "        new_columns[f\"south_w1[{n}]\"] = safe_sum(draws, south_w1)\n",
    "        new_columns[f\"west_w1[{n}]\"] = safe_sum(draws, west_w1)\n",
    "\n",
    "        new_columns[f\"north_w2[{n}]\"] = safe_sum(draws, north_w2)\n",
    "        new_columns[f\"east_w2[{n}]\"] = safe_sum(draws, east_w2)\n",
    "        new_columns[f\"south_w2[{n}]\"] = safe_sum(draws, south_w2)\n",
    "        new_columns[f\"west_w2[{n}]\"] = safe_sum(draws, west_w2)\n",
    "\n",
    "        new_columns[f\"north_p[{n}]\"] = safe_sum(draws, north_p)\n",
    "        new_columns[f\"east_p[{n}]\"] = safe_sum(draws, east_p)\n",
    "        new_columns[f\"south_p[{n}]\"] = safe_sum(draws, south_p)\n",
    "        new_columns[f\"west_p[{n}]\"] = safe_sum(draws, west_p)\n",
    "        new_columns[f\"vert_p[{n}]\"] = safe_sum(draws, vert_p)\n",
    "\n",
    "    processed_draws = pd.concat([processed_draws, pd.DataFrame(new_columns)], axis=1)\n",
    "    return processed_draws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_names = [\"tot_w1\", \"tot_abs_w1\", \"north_w1\", \"east_w1\", \"south_w1\", \"west_w1\",\n",
    "                     \"tot_w2\", \"tot_abs_w2\", \"north_w2\", \"east_w2\", \"south_w2\", \"west_w2\",\n",
    "                     \"tot_p\", \"tot_abs_p\", \"north_p\", \"east_p\", \"south_p\", \"west_p\", \"vert_p\"]\n",
    "\n",
    "correlation_labels = {\"tot_w1\": r\"location mean $\\omega^1$\",\n",
    "                      \"tot_abs_w1\": r\"location absolute mean $\\omega^1$\",\n",
    "                      \"north_w1\": r\"northward edge $\\omega^1$\",\n",
    "                      \"east_w1\": r\"eastward edge $\\omega^1$\",\n",
    "                      \"south_w1\": r\"southward edge $\\omega^1$\",\n",
    "                      \"west_w1\": r\"westward edge $\\omega^1$\",\n",
    "                      \"tot_w2\": r\"location mean $\\omega^2$\",\n",
    "                      \"tot_abs_w2\": r\"location absolute mean $\\omega^2$\",\n",
    "                      \"north_w2\": r\"northward edge $\\omega^2$\",\n",
    "                      \"east_w2\": r\"eastward edge $\\omega^2$\",\n",
    "                      \"south_w2\": r\"southward edge $\\omega^2$\",\n",
    "                      \"west_w2\": r\"westward edge $\\omega^2$\",\n",
    "                      \"tot_p\": r\"location mean $\\rho$\",\n",
    "                      \"tot_abs_p\": r\"location absolute mean $\\rho$\",\n",
    "                      \"north_p\": r\"northward edge $\\rho$\",\n",
    "                      \"east_p\": r\"eastward edge $\\rho$\",\n",
    "                      \"south_p\": r\"southward edge $\\rho$\",\n",
    "                      \"west_p\": r\"westward edge $\\rho$\",\n",
    "                      \"vert_p\": r\"vertical (between-layer) edge $\\rho$\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the run configurations\n",
    "run_configs = [\n",
    "    {\"model_n\": 19, \"sparse\": 1,  \"time_range\": (1, 3)},\n",
    "    {\"model_n\": 19, \"sparse\": 1,  \"time_range\": (83, 85)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_figure(model_n, sparse, time_steps):\n",
    "    # Input Files\n",
    "    if sparse:\n",
    "        draws_filename = f\"/Volumes/KINGSTON/M4R_usb/R/modular_long/nm{model_n}[{time_steps[0]}-{time_steps[1]}]_sparse_draws.csv\"\n",
    "    else:\n",
    "        draws_filename = f\"/Volumes/KINGSTON/M4R_usb/R/modular_long/nm{model_n}[{time_steps[0]}-{time_steps[1]}]_draws.csv\"\n",
    "\n",
    "    if sparse:\n",
    "        edge_map_filenames = [f\"R/notebook_models/[{time_steps[0]}-{time_steps[1]}]_sparse_edge_map.csv\",\n",
    "                            f\"R/notebook_models/[{time_steps[0]}-{time_steps[1]}]_sparse_edge_diag_map.csv\"]\n",
    "    else:\n",
    "        edge_map_filenames = [\"R/notebook_models/edge_map.csv\",\n",
    "                            \"R/notebook_models/edge_diag_map.csv\"]\n",
    "\n",
    "    # Out file\n",
    "    if sparse:\n",
    "        out_file = f\"/Volumes/KINGSTON/M4R_usb/R/modular/summary_figures/nm{model_n}[{time_steps[0]}-{time_steps[1]}]_sparse_map_small.pdf\"\n",
    "    else:\n",
    "        out_file = f\"/Volumes/KINGSTON/M4R_usb/R/modular/summary_figures/nm{model_n}[{time_steps[0]}-{time_steps[1]}]_map_small.pdf\"\n",
    "\n",
    "    # Load the maps\n",
    "    edge_map_df = pd.read_csv(edge_map_filenames[0])\n",
    "    edge_diag_map_df = pd.read_csv(edge_map_filenames[1])\n",
    "\n",
    "    edge_map = dict(zip(edge_map_df[\"edge_id\"], edge_map_df[\"edge_key\"]))\n",
    "    edge_diag_map = dict(zip(edge_diag_map_df[\"edge_id\"], edge_diag_map_df[\"edge_key\"]))\n",
    "\n",
    "    # Read and Process Draws\n",
    "    draws = pd.read_csv(draws_filename)\n",
    "    draws.columns = [rename_param(col, edge_map, edge_diag_map) for col in draws.columns]\n",
    "    processed_draws = process_draws(draws)\n",
    "\n",
    "    # Define grid size\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(13, 5), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "\n",
    "    # Define lat/lon extent\n",
    "    extent = [-84.125, -79.875, 24.875, 29.125]\n",
    "    latitudes = np.linspace(24.875, 29.125, num_lat)\n",
    "    longitudes = np.linspace(-84.125, -79.875, num_lat)\n",
    "\n",
    "    indices = [3, 15, 18]\n",
    "    correlation_names_ = [correlation_names[i] for i in indices]\n",
    "    #correlation_names_ = np.array(correlation_names_[:-1]).reshape((3, 2)).T.flatten().tolist() + [None, None, correlation_names_[-1]]\n",
    "\n",
    "    draw = 1000\n",
    "    mesh = None\n",
    "    # Iterate over correlation names and plot in subplots\n",
    "    for ax, correlation_name in zip(axes.flat, correlation_names_):\n",
    "        if correlation_name is None:\n",
    "            ax.axis('off')  # Hide unused subplot\n",
    "            continue\n",
    "        t1_data = processed_draws.loc[draw-500:draw, [c for c in processed_draws.columns if c.startswith(correlation_name)]]\n",
    "        t2_data = zone_data.copy()\n",
    "        t2_data[correlation_name] = list(np.mean(t1_data.loc[draw-200:draw, :], axis=0))\n",
    "\n",
    "        heatmap_data = t2_data.pivot(index='latitude_rank', columns='longitude_rank', values=correlation_name).sort_index(ascending=False)\n",
    "\n",
    "        # Plot using imshow\n",
    "        mesh = ax.imshow(heatmap_data, extent=extent, origin=\"upper\", cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "\n",
    "        # Add coastline\n",
    "        ax.add_feature(cfeature.COASTLINE, edgecolor='black')\n",
    "\n",
    "        # Set title\n",
    "        ax.set_title(correlation_labels[correlation_name], fontsize=12)\n",
    "\n",
    "    # Adjust layout and add colorbar\n",
    "    fig.subplots_adjust(right=0.85)\n",
    "    cbar_ax = fig.add_axes([0.87, 0.15, 0.02, 0.7])  # Colorbar position\n",
    "    fig.colorbar(mesh, cax=cbar_ax, orientation='vertical', label=\"Edge Strength Value\").ax.set_ylabel(\"Edge Strength Value\", fontsize=15)\n",
    "    \n",
    "    nm_model = {19: \"Model 1\",\n",
    "                21: \"Model 2\",\n",
    "                22: \"Model 3\",\n",
    "                23: \"Model 4\",\n",
    "                12: r\"Model 1 ($\\boldsymbol{\\phi}$ removed)\",\n",
    "                17: r\"Model 2 ($\\boldsymbol{\\phi}$ removed)\",\n",
    "                15: r\"Model 3 ($\\boldsymbol{\\phi}$ removed)\",\n",
    "                16: r\"Model 4 ($\\boldsymbol{\\phi}$ removed)\"}\n",
    "    label_sparse = {1: \"Filtered\", 0: \"Unfiltered\"}\n",
    "    plt.suptitle(f\"Heatmaps of Edge Strength Posterior Means ({nm_model[model_n]}, {label_sparse[sparse]}, Time Steps {time_steps[0]}-{time_steps[1]})\", fontsize=16,\n",
    "                x=0.5, y=0.94)\n",
    "\n",
    "    #plt.savefig(out_file, format=\"pdf\", bbox_inches=\"tight\", dpi=300)\n",
    "    # plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the loop\n",
    "run_configs = [\n",
    "    {\"model_n\": 22, \"sparse\": 0,  \"time_range\": (83, 85)},\n",
    "    {\"model_n\": 23, \"sparse\": 0,  \"time_range\": (83, 85)}\n",
    "    ]\n",
    "\n",
    "for config in run_configs[:]:\n",
    "    summary_figure(config[\"model_n\"], config[\"sparse\"], config[\"time_range\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def small_summary_comparison_figure(config1, config2):\n",
    "    configs = [config1, config2]\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(13, 8), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    extent = [-84.125, -79.875, 24.875, 29.125]\n",
    "\n",
    "    indices = [3, 9, 15]\n",
    "    correlation_names_ = [correlation_names[i] for i in indices]\n",
    "    mesh = None\n",
    "\n",
    "    # For labeling\n",
    "    nm_model = {19: \"Model 1\", 21: \"Model 2\", 22: \"Model 3\", 23: \"Model 4\",\n",
    "                12: r\"Model 1 ($\\boldsymbol{\\phi}$ removed)\", 17: r\"Model 2 ($\\boldsymbol{\\phi}$ removed)\",\n",
    "                15: r\"Model 3 ($\\boldsymbol{\\phi}$ removed)\", 16: r\"Model 4 ($\\boldsymbol{\\phi}$ removed)\"}\n",
    "    label_sparse = {1: \"Filtered\", 0: \"Unfiltered\"}\n",
    "\n",
    "    for row_idx, config in enumerate(configs):\n",
    "        model_n = config[\"model_n\"]\n",
    "        sparse = config[\"sparse\"]\n",
    "        time_steps = config[\"time_range\"]\n",
    "\n",
    "        if sparse:\n",
    "            draws_filename = f\"/Volumes/KINGSTON/M4R_usb/R/modular/nm{model_n}[{time_steps[0]}-{time_steps[1]}]_sparse_draws.csv\"\n",
    "            edge_map_filenames = [f\"R/notebook_models/[{time_steps[0]}-{time_steps[1]}]_sparse_edge_map.csv\",\n",
    "                                  f\"R/notebook_models/[{time_steps[0]}-{time_steps[1]}]_sparse_edge_diag_map.csv\"]\n",
    "        else:\n",
    "            draws_filename = f\"/Volumes/KINGSTON/M4R_usb/R/modular/nm{model_n}[{time_steps[0]}-{time_steps[1]}]_draws.csv\"\n",
    "            edge_map_filenames = [\"R/notebook_models/edge_map.csv\",\n",
    "                                  \"R/notebook_models/edge_diag_map.csv\"]\n",
    "\n",
    "        edge_map_df = pd.read_csv(edge_map_filenames[0])\n",
    "        edge_diag_map_df = pd.read_csv(edge_map_filenames[1])\n",
    "        edge_map = dict(zip(edge_map_df[\"edge_id\"], edge_map_df[\"edge_key\"]))\n",
    "        edge_diag_map = dict(zip(edge_diag_map_df[\"edge_id\"], edge_diag_map_df[\"edge_key\"]))\n",
    "\n",
    "        draws = pd.read_csv(draws_filename)\n",
    "        draws.columns = [rename_param(col, edge_map, edge_diag_map) for col in draws.columns]\n",
    "        processed_draws = process_draws(draws)\n",
    "\n",
    "        draw = 1000\n",
    "\n",
    "        for col_idx, correlation_name in enumerate(correlation_names_):\n",
    "            ax = axes[row_idx, col_idx]\n",
    "            t1_data = processed_draws.loc[draw-500:draw, [c for c in processed_draws.columns if c.startswith(correlation_name)]]\n",
    "            t2_data = zone_data.copy()\n",
    "            t2_data[correlation_name] = list(np.mean(t1_data.loc[draw-100:draw, :], axis=0))\n",
    "            heatmap_data = t2_data.pivot(index='latitude_rank', columns='longitude_rank', values=correlation_name).sort_index(ascending=False)\n",
    "\n",
    "            mesh = ax.imshow(heatmap_data, extent=extent, origin=\"upper\", cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "            ax.add_feature(cfeature.COASTLINE, edgecolor='black')\n",
    "            ax.set_title(correlation_labels[correlation_name], fontsize=13)\n",
    "\n",
    "    # Add colorbar\n",
    "    fig.subplots_adjust(right=0.85)\n",
    "    cbar_ax = fig.add_axes([0.87, 0.15, 0.02, 0.7])\n",
    "    fig.colorbar(mesh, cax=cbar_ax, orientation='vertical', label=\"Edge Strength Value\").ax.set_ylabel(\"Edge Strength Value\", fontsize=13)\n",
    "\n",
    "    # Row labels\n",
    "    label1 = f\"Time Range: {config1['time_range'][0]}–{config1['time_range'][1]}\"\n",
    "    label2 = f\"Time Range: {config2['time_range'][0]}–{config2['time_range'][1]}\"\n",
    "    fig.text(0.11, 0.71, label1, rotation='vertical', va='center', ha='center', fontsize=13)\n",
    "    fig.text(0.11, 0.29, label2, rotation='vertical', va='center', ha='center', fontsize=13)\n",
    "\n",
    "    # Title\n",
    "    plt.suptitle(\"Model 2 Inferred Posterior Means on Sparse Data\", fontsize=16, y=0.97)\n",
    "    # plt.tight_layout()\n",
    "    plt.savefig(\n",
    "         f\"/Volumes/KINGSTON/M4R_usb/R/modular/summary_figures/\"\n",
    "         f\"Model2_sparse.pdf\",\n",
    "         dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the loop\n",
    "run_configs = [\n",
    "    {\"model_n\": 21, \"sparse\": 1,  \"time_range\": (1, 3)},\n",
    "    {\"model_n\": 21, \"sparse\": 1,  \"time_range\": (83, 85)}\n",
    "    ]\n",
    "small_summary_comparison_figure(run_configs[0], run_configs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "model_n = 21\n",
    "sparse = 1\n",
    "time_steps = (83, 85)\n",
    "\n",
    "# File path\n",
    "if sparse:\n",
    "    summary_filename = f\"/Volumes/KINGSTON/M4R_usb/R/modular/nms{model_n}[{time_steps[0]}-{time_steps[1]}]_sparse.csv\"\n",
    "    output_filename = f\"/Volumes/KINGSTON/M4R_usb/R/modular/nms{model_n}[{time_steps[0]}-{time_steps[1]}]_sparse_cut.csv\"\n",
    "else:\n",
    "    summary_filename = f\"/Volumes/KINGSTON/M4R_usb/R/modular/nms{model_n}[{time_steps[0]}-{time_steps[1]}].csv\"\n",
    "    output_filename = f\"/Volumes/KINGSTON/M4R_usb/R/modular/nms{model_n}[{time_steps[0]}-{time_steps[1]}]_cut.csv\"\n",
    "\n",
    "# Read data\n",
    "df = pd.read_csv(summary_filename)\n",
    "\n",
    "# Always include lp__\n",
    "filtered_df = df[df[\"variable\"] == \"lp__\"]\n",
    "\n",
    "# Function to get first 10 from group where rhat < 1.01\n",
    "def filter_group(prefix):\n",
    "    return df[df[\"variable\"].str.startswith(prefix)].head(5)\n",
    "\n",
    "# Append filtered results for each variable group\n",
    "for prefix in [\"w1[\", \"w2[\", \"p[\", \"phi[\"]:\n",
    "    filtered_df = pd.concat([filtered_df, filter_group(prefix)], axis=0)\n",
    "\n",
    "# Reset index and display result\n",
    "filtered_df = filtered_df.reset_index(drop=True)\n",
    "filtered_df.to_csv(output_filename, index=False)\n",
    "print(filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nm_model = {\n",
    "    19: \"Model 1\",\n",
    "    21: \"Model 2\",\n",
    "    22: \"Model 3\",\n",
    "    23: \"Model 4\",\n",
    "    12: r\"Model 1 ($\\boldsymbol{\\phi}$ removed)\",\n",
    "    17: r\"Model 2 ($\\boldsymbol{\\phi}$ removed)\",\n",
    "    15: r\"Model 3 ($\\boldsymbol{\\phi}$ removed)\",\n",
    "    16: r\"Model 4 ($\\boldsymbol{\\phi}$ removed)\",\n",
    "}\n",
    "label_sparse = {1: \"Filtered\", 0: \"Unfiltered\"}\n",
    "\n",
    "def plot_correlation_comparison(correlation_name, configs):\n",
    "    \"\"\"\n",
    "    Produce a 1×3 row of heatmaps for a single correlation_name, using three\n",
    "    different (model_n, sparse, time_range) configs.\n",
    "    \n",
    "    - configs: a list of exactly three dicts, each with keys:\n",
    "        { \"model_n\": int, \"sparse\": 0 or 1, \"time_range\": (start, end) }\n",
    "    - correlation_name: e.g. \"tot_w1[5]\" (must match entries in processed_draws)\n",
    "    \"\"\"\n",
    "    assert len(configs) == 3, \"Provide exactly three configurations.\"\n",
    "\n",
    "    # Create 1×3 figure\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=1, ncols=3, figsize=(15, 5),\n",
    "        subplot_kw={'projection': ccrs.PlateCarree()}\n",
    "    )\n",
    "\n",
    "    # Colorbar will be shared; remember last 'mesh'\n",
    "    mesh = None\n",
    "\n",
    "    # Define lat/lon extent once\n",
    "    extent = [-84.125, -79.875, 24.875, 29.125]\n",
    "\n",
    "    for ax, cfg in zip(axes.flat, configs):\n",
    "        model_n = cfg[\"model_n\"]\n",
    "        sparse   = cfg[\"sparse\"]\n",
    "        ts0, ts1 = cfg[\"time_range\"]\n",
    "\n",
    "        # 1) Build the filenames\n",
    "        if sparse:\n",
    "            draws_filename = (f\"/Volumes/KINGSTON/M4R_usb/R/modular/\"\n",
    "                              f\"nm{model_n}[{ts0}-{ts1}]_sparse_draws.csv\")\n",
    "            edge_map_filenames = [\n",
    "                f\"R/notebook_models/[{ts0}-{ts1}]_sparse_edge_map.csv\",\n",
    "                f\"R/notebook_models/[{ts0}-{ts1}]_sparse_edge_diag_map.csv\"\n",
    "            ]\n",
    "        else:\n",
    "            draws_filename = (f\"/Volumes/KINGSTON/M4R_usb/R/modular/\"\n",
    "                              f\"nm{model_n}[{ts0}-{ts1}]_draws.csv\")\n",
    "            edge_map_filenames = [\n",
    "                \"R/notebook_models/edge_map.csv\",\n",
    "                \"R/notebook_models/edge_diag_map.csv\"\n",
    "            ]\n",
    "\n",
    "        # 2) Load edge‐maps, build dicts\n",
    "        edge_map_df      = pd.read_csv(edge_map_filenames[0])\n",
    "        edge_diag_map_df = pd.read_csv(edge_map_filenames[1])\n",
    "        edge_map      = dict(zip(edge_map_df[\"edge_id\"],      edge_map_df[\"edge_key\"]))\n",
    "        edge_diag_map = dict(zip(edge_diag_map_df[\"edge_id\"], edge_diag_map_df[\"edge_key\"]))\n",
    "\n",
    "        # 3) Read draws, rename columns, process\n",
    "        draws = pd.read_csv(draws_filename)\n",
    "        draws.columns = [rename_param(col, edge_map, edge_diag_map)\n",
    "                         for col in draws.columns]\n",
    "        processed_draws = process_draws(draws)\n",
    "\n",
    "        # 4) Compute the mean of the last 100 iterations for 'correlation_name'\n",
    "        #    over iterations draw-100 … draw. We fix draw=1000 as before.\n",
    "        draw_idx = 1000\n",
    "        all_cols = [c for c in processed_draws.columns\n",
    "                    if c.startswith(correlation_name)]\n",
    "        # take iterations [draw_idx-100 : draw_idx]\n",
    "        t1_data = processed_draws.loc[draw_idx-100 : draw_idx, all_cols]\n",
    "        t2_data = zone_data.copy()\n",
    "        t2_data[correlation_name] = t1_data.mean(axis=0).values\n",
    "\n",
    "        # 5) Pivot into a heatmap (lat × lon grid)\n",
    "        heatmap_data = (\n",
    "            t2_data\n",
    "            .pivot(index='latitude_rank',\n",
    "                   columns='longitude_rank',\n",
    "                   values=correlation_name)\n",
    "            .sort_index(ascending=False)\n",
    "        )\n",
    "\n",
    "        # 6) Plot\n",
    "        mesh = ax.imshow(\n",
    "            heatmap_data,\n",
    "            extent=extent,\n",
    "            origin=\"upper\",\n",
    "            cmap=\"coolwarm\",\n",
    "            vmin=-1, vmax=1\n",
    "        )\n",
    "        ax.add_feature(cfeature.COASTLINE, edgecolor='black')\n",
    "\n",
    "        # 7) Title each subplot with model/sparse/time info\n",
    "        title = (f\"{nm_model[model_n]} — {label_sparse[sparse]}\\n\"\n",
    "                 f\"Time Steps {ts0}–{ts1}\")\n",
    "        ax.set_title(title, fontsize=15)\n",
    "\n",
    "    # 8) Adjust layout and add a single vertical colorbar on the right\n",
    "    fig.subplots_adjust(right=0.85, wspace=0.1)\n",
    "    cbar_ax = fig.add_axes([0.88, 0.15, 0.02, 0.7])  # [left, bottom, width, height]\n",
    "    cbar = fig.colorbar(mesh, cax=cbar_ax, orientation='vertical')\n",
    "    cbar.set_label(\"Edge Strength Value\", fontsize=14)\n",
    "    cbar.ax.tick_params(labelsize=10)\n",
    "\n",
    "    # 9) Super‐title for the entire row\n",
    "    shared_title = (f\"Heatmap of '{correlation_labels[correlation_name]}' Posterior Means\\n\"\n",
    "                    f\"({nm_model[configs[0]['model_n']]}, {configs[0]['time_range'][0]}–{configs[0]['time_range'][1]}) | \"\n",
    "                    f\"({nm_model[configs[1]['model_n']]}, {configs[1]['time_range'][0]}–{configs[1]['time_range'][1]}) | \"\n",
    "                    f\"({nm_model[configs[2]['model_n']]}, {configs[2]['time_range'][0]}–{configs[2]['time_range'][1]})\")\n",
    "    plt.suptitle(shared_title, fontsize=16, x=0.5, y=1.1)\n",
    "    # plt.show()\n",
    "    # 10) Save or show\n",
    "    #plt.savefig(\n",
    "    #     f\"/Volumes/KINGSTON/M4R_usb/R/modular/summary_figures/\"\n",
    "    #     f\"{correlation_name.replace('[','').replace(']','')}_comparison.pdf\",\n",
    "    #     dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_configs = [\n",
    "    {\"model_n\": 19, \"sparse\": 0,  \"time_range\": (1, 3)},\n",
    "    {\"model_n\": 19, \"sparse\": 0,  \"time_range\": (83, 85)},\n",
    "    {\"model_n\": 19, \"sparse\": 0,  \"time_range\": (100, 103)}]\n",
    "\n",
    "plot_correlation_comparison(\"east_p\", run_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_correlation_heatmaps(configs, correlation_names):\n",
    "    \"\"\"\n",
    "    Plots a 2x2 grid of heatmaps comparing two correlation names across two model configurations.\n",
    "\n",
    "    Args:\n",
    "        configs: A list of two dictionaries, each with keys 'model_n', 'sparse', and 'time_range'.\n",
    "        correlation_names: A list of two correlation name strings.\n",
    "    \"\"\"\n",
    "    assert len(configs) == 2 and len(correlation_names) == 2, \"Provide exactly two configs and two correlation names.\"\n",
    "\n",
    "    extent = [-84.125, -79.875, 24.875, 29.125]\n",
    "    draw = 1000\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(8, 7), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    mesh = None  # For colorbar\n",
    "\n",
    "    for col, config in enumerate(configs):\n",
    "        model_n = config[\"model_n\"]\n",
    "        sparse = config[\"sparse\"]\n",
    "        time_steps = config[\"time_range\"]\n",
    "\n",
    "        # Load draws and maps\n",
    "        if sparse:\n",
    "            draws_filename = f\"/Volumes/KINGSTON/M4R_usb/R/modular/nm{model_n}[{time_steps[0]}-{time_steps[1]}]_sparse_draws.csv\"\n",
    "            edge_map_filenames = [f\"R/notebook_models/[{time_steps[0]}-{time_steps[1]}]_sparse_edge_map.csv\",\n",
    "                                  f\"R/notebook_models/[{time_steps[0]}-{time_steps[1]}]_sparse_edge_diag_map.csv\"]\n",
    "        else:\n",
    "            draws_filename = f\"/Volumes/KINGSTON/M4R_usb/R/modular/nm{model_n}[{time_steps[0]}-{time_steps[1]}]_draws.csv\"\n",
    "            edge_map_filenames = [\"R/notebook_models/edge_map.csv\", \"R/notebook_models/edge_diag_map.csv\"]\n",
    "\n",
    "        edge_map_df = pd.read_csv(edge_map_filenames[0])\n",
    "        edge_diag_map_df = pd.read_csv(edge_map_filenames[1])\n",
    "        edge_map = dict(zip(edge_map_df[\"edge_id\"], edge_map_df[\"edge_key\"]))\n",
    "        edge_diag_map = dict(zip(edge_diag_map_df[\"edge_id\"], edge_diag_map_df[\"edge_key\"]))\n",
    "\n",
    "        draws = pd.read_csv(draws_filename)\n",
    "        draws.columns = [rename_param(col, edge_map, edge_diag_map) for col in draws.columns]\n",
    "        processed_draws = process_draws(draws)\n",
    "\n",
    "        for row, corr_name in enumerate(correlation_names):\n",
    "            ax = axes[row, col]\n",
    "            t1_data = processed_draws.loc[draw-500:draw, [c for c in processed_draws.columns if c.startswith(corr_name)]]\n",
    "            t2_data = zone_data.copy()\n",
    "            t2_data[corr_name] = list(np.mean(t1_data.loc[draw-100:draw, :], axis=0))\n",
    "\n",
    "            heatmap_data = t2_data.pivot(index='latitude_rank', columns='longitude_rank', values=corr_name).sort_index(ascending=False)\n",
    "            mesh = ax.imshow(heatmap_data, extent=extent, origin=\"upper\", cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "            ax.add_feature(cfeature.COASTLINE, edgecolor='black')\n",
    "\n",
    "            title = correlation_labels.get(corr_name, corr_name)\n",
    "            model_label = f\"{correlation_labels.get(corr_name, corr_name)} ({nm_model[model_n]}, {'Filtered' if sparse else 'Unfiltered'})\"\n",
    "            ax.set_title(model_label, fontsize=10)\n",
    "\n",
    "    # Adjust layout\n",
    "    fig.subplots_adjust(right=0.9, hspace=0.3)\n",
    "    cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "    fig.colorbar(mesh, cax=cbar_ax, orientation='vertical', label=\"Edge Strength Value\")\n",
    "\n",
    "    plt.suptitle(\"Model 1 and Model 2 Comparison\", fontsize=14, y=0.96)\n",
    "    plt.savefig(\n",
    "         f\"/Volumes/KINGSTON/M4R_usb/R/modular/summary_figures/\"\n",
    "         f\"Model12[83-85]_comparison.pdf\",\n",
    "         dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_configs = [\n",
    "    {\"model_n\": 19, \"sparse\": 0,  \"time_range\": (83, 85)},\n",
    "    {\"model_n\": 21, \"sparse\": 0,  \"time_range\": (83, 85)}]\n",
    "cn = [\"east_w2\", \"east_p\"]\n",
    "compare_correlation_heatmaps(run_configs, cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "M4R_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
