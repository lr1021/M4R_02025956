{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cfgrib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"4_all_data_grib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable extraction file\n",
    "var_file = input_path + \"/zone_0/c3_ABLE_1950_08.grib\"\n",
    "\n",
    "# Load metadata using cfgrib\n",
    "ds = cfgrib.open_datasets(var_file, cache=False)\n",
    "paramID_list = []\n",
    "shortName_list = []\n",
    "Name_list = []\n",
    "\n",
    "# Display available parameter IDs and their variables\n",
    "for dataset in ds:\n",
    "    for var_name in dataset.variables:\n",
    "        if ('GRIB_paramId' in dataset[var_name].attrs.keys()):\n",
    "            shortName_list.append(var_name)\n",
    "            paramID_list.append(dataset[var_name].attrs['GRIB_paramId'])\n",
    "            Name_list.append(dataset[var_name].attrs['GRIB_name'])\n",
    "            print(f\"{var_name}: {dataset[var_name].attrs['GRIB_name']}\")\n",
    "            print(f\"{var_name}: {dataset[var_name].attrs['GRIB_paramId']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_to_name = dict(zip(paramID_list, shortName_list))\n",
    "ID_to_long_name = dict(zip(shortName_list, Name_list))\n",
    "for key, value in ID_to_long_name.items():\n",
    "    print(f\"{key} : {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_grib(filepath):\n",
    "    # Open the GRIB file\n",
    "    grib_file = filepath\n",
    "    valid_time_min_list = []\n",
    "    valid_time_max_list = []\n",
    "\n",
    "    for p in paramID_list[:]:\n",
    "        ds = cfgrib.open_dataset(\n",
    "            grib_file,\n",
    "            filter_by_keys= {'paramId':p}\n",
    "        )\n",
    "        df = ds.to_dataframe().reset_index()[['valid_time']];\n",
    "        valid_time_min_list.append(min(df['valid_time']))\n",
    "        valid_time_max_list.append(max(df['valid_time']))\n",
    "    valid_time_min = max(valid_time_min_list)\n",
    "    valid_time_max = min(valid_time_max_list)\n",
    "\n",
    "    # Step 2: Reopen datasets filtered by the common valid_time range\n",
    "    filtered_dataframes = []\n",
    "\n",
    "    for p in paramID_list[:]:\n",
    "        ds = cfgrib.open_dataset(\n",
    "            grib_file,\n",
    "            filter_by_keys={'paramId': p}\n",
    "        )\n",
    "        df = ds.to_dataframe().reset_index()[['valid_time', 'latitude', 'longitude', ID_to_name[p]]]\n",
    "        \n",
    "        # Filter dataframe by the common valid_time range\n",
    "        df_filtered = df[(df['valid_time'] >= valid_time_min) &\n",
    "                        (df['valid_time'] <= valid_time_max)].sort_values(['valid_time', 'latitude', 'longitude']).reset_index(drop=True)\n",
    "        filtered_dataframes.append(df_filtered)\n",
    "\n",
    "    # Step 3: Merge the filtered dataframes on common keys\n",
    "    final_df = pd.concat(filtered_dataframes, axis=1)\n",
    "\n",
    "    # Drop duplicate columns after merging\n",
    "    final_df = final_df.loc[:, ~final_df.columns.duplicated()]\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace = False\n",
    "# CSV\n",
    "file_count = 1e5\n",
    "file_count_i = 0\n",
    "\n",
    "# Input and output directories\n",
    "outer_input_folder = \"4_all_data_grib\"\n",
    "outer_output_folder = \"6_all_data_table\"\n",
    "\n",
    "for zone in [0, 1, 2]:\n",
    "    # Ensure output directory exists\n",
    "    inner_input_folder = outer_input_folder + \"/zone_\" + f\"{zone}\"\n",
    "    inner_output_folder = outer_output_folder + \"/zone_\" + f\"{zone}\"\n",
    "    os.makedirs(inner_output_folder, exist_ok=True)\n",
    "\n",
    "    # Loop through all GRIB files in the input folder\n",
    "    for filename in os.listdir(inner_input_folder):\n",
    "\n",
    "        if file_count_i==file_count:\n",
    "            break\n",
    "        file_count_i += 1\n",
    "        \n",
    "        if filename.endswith(\".grib\"):  # Process only .grib files\n",
    "            grib_file_path = os.path.join(inner_input_folder, filename)\n",
    "            # Define the output CSV file path\n",
    "            csv_filename = os.path.splitext(filename)[0] + \".csv\"\n",
    "            csv_file_path = os.path.join(inner_output_folder, csv_filename)\n",
    "            \n",
    "            if ((not replace) and (os.path.exists(csv_file_path))):\n",
    "                print(f\"Already exists: {csv_file_path}\")\n",
    "            else:\n",
    "                try:\n",
    "                    print(f\"Processing: {csv_file_path}\")\n",
    "                    df = process_grib(grib_file_path)\n",
    "                    # Save the dataframe as a CSV file\n",
    "                    df.to_csv(csv_file_path, index=False)\n",
    "                    print(f\"Processed and saved: {csv_file_path}\")\n",
    "                    \n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing: {filename} {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone = 0\n",
    "input_folder = f\"6_all_data_table/zone_{zone}\"\n",
    "\n",
    "###\n",
    "filename=os.listdir(input_folder)[0]\n",
    "data = pd.read_csv(input_folder+\"/\"+filename)[['latitude', 'longitude']]\n",
    "data = data.drop_duplicates()\n",
    "\n",
    "lat = data['latitude'].unique()\n",
    "lon = data['longitude'].unique()\n",
    "lat_ind = np.arange(1, len(lat)+1)\n",
    "lon_ind = np.arange(1, len(lon)+1)\n",
    "lat_dict = {l:i for l,i in zip(lat,lat_ind)}\n",
    "lon_dict = {l:i for l,i in zip(lon,lon_ind)}\n",
    "data['latitude_rank'] = [lat_dict[l] for l in data['latitude']]\n",
    "data['longitude_rank'] = [lon_dict[l] for l in data['longitude']]\n",
    "\n",
    "data = data.sort_values(by=['longitude', 'latitude'], ascending=[True, True]).reset_index(drop=True)\n",
    "data['index'] = data.index+1\n",
    "latitude_rank = np.array(data['latitude_rank'])\n",
    "longitude_rank = np.array(data['longitude_rank'])\n",
    "\n",
    "lat_to_rank = pd.Series(data.latitude_rank.values, index = data.latitude).to_dict()\n",
    "long_to_rank = pd.Series(data.longitude_rank.values, index = data.longitude).to_dict()\n",
    "coord_to_index = pd.Series(data.index, index = zip(data.latitude, data.longitude)).to_dict()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(input_folder):\n",
    "    filename = input_folder + \"/\" + filename\n",
    "    event_data = pd.read_csv(filename)\n",
    "\n",
    "    current_columns = event_data.columns\n",
    "    if 'latitude_rank' not in current_columns:\n",
    "        event_data['latitude_rank'] = [lat_to_rank[lat] for lat in event_data['latitude']]\n",
    "\n",
    "    if 'longitude_rank' not in current_columns:\n",
    "        event_data['longitude_rank'] = [long_to_rank[long] for long in event_data['longitude']]\n",
    "\n",
    "    if 'coord_index' not in current_columns:\n",
    "        event_data['coord_index'] = [coord_to_index[c] for c in zip(event_data['latitude'], event_data['longitude'])]\n",
    "\n",
    "    if 'time_step' not in current_columns:\n",
    "        valid_time_tp = pd.to_datetime(event_data['valid_time'])\n",
    "        # Find the earliest date\n",
    "        earliest_date = valid_time_tp.min()\n",
    "        # Compute the number of timesteps from the earliest entry (hourly data)\n",
    "        event_data['time_step'] = ((valid_time_tp - earliest_date).dt.total_seconds() / 3600).astype(int) + 1\n",
    "\n",
    "    event_data['w10'] = np.sqrt(event_data['u10']**2 + event_data['v10']**2)\n",
    "    event_data['w100'] = np.sqrt(event_data['u100']**2 + event_data['v100']**2)\n",
    "\n",
    "    event_data = event_data[['valid_time', 'time_step', 'latitude', 'latitude_rank',\n",
    "                                 'longitude', 'longitude_rank', 'coord_index', 'u10', 'u100', 'v10', 'v100',\n",
    "                                 'w10', 'w100', 'tp', 't2m', 'lsm', 'sp', 'tcw', 'e']]\n",
    "\n",
    "    event_data.to_csv(filename, index=False)\n",
    "\n",
    "print(\"Finished adding time_step, coordinate rank and index to data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(input_folder):\n",
    "    filename = input_folder + \"/\" + filename\n",
    "    columns = pd.read_csv(filename, index_col=None, nrows=0).columns.tolist()\n",
    "    wanted = ['valid_time', 'time_step', 'latitude', 'latitude_rank',\n",
    "              'longitude', 'longitude_rank', 'coord_index', 'u10', 'u100', 'v10', 'v100',\n",
    "              'w10', 'w100', 'tp', 't2m', 'lsm', 'sp', 'tcw', 'e']\n",
    "    missing = list(set(columns) - set(wanted))\n",
    "    if columns != wanted:\n",
    "        print(filename, missing)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "M4R_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
